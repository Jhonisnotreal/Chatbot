{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Labels\n",
    "\n",
    "### Well, let's start by training some data and labels to our language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    \"Hello\",\n",
    "    \"How are you?\",\n",
    "    \"Good morning\",\n",
    "    \"Good evening\",\n",
    "    \"Nice to meet you\",\n",
    "    \"What's up?\",\n",
    "    \"How's your day going?\",\n",
    "    \"Greetings!\",\n",
    "    \"Good afternoon\",\n",
    "    \"How can I assist you?\",\n",
    "    \"Pleasure to see you\",\n",
    "    \"Is there anything I can help with?\"\n",
    "]\n",
    "\n",
    "train_labels = [\n",
    "    \"Hi\",\n",
    "    \"I'm fine, how about you?\",\n",
    "    \"Good morning to you\",\n",
    "    \"Good evening, how can I help you?\",\n",
    "    \"Nice to meet you too\",\n",
    "    \"Not much, just hanging out\",\n",
    "    \"It's going well, thank you\",\n",
    "    \"Hello!\",\n",
    "    \"Good afternoon to you too\",\n",
    "    \"I'm here to assist you\",\n",
    "    \"Likewise!\",\n",
    "    \"Yes, I have a question\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's a little small but this is for a good reason, and the reason is for educational purposes and computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "train_sequences = keras.preprocessing.sequence.pad_sequences(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential groups a linear stack of layers into a tf.keras.Model.\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Embedding(len(tokenizer.word_index) + 1, 100, input_length=train_sequences.shape[1]))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(len(train_labels), activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_sequences, encoded_labels, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_response(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    sequence = keras.preprocessing.sequence.pad_sequences(sequence, maxlen=train_sequences.shape[1])\n",
    "    prediction = model.predict(sequence)\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    response = label_encoder.inverse_transform([predicted_label])[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"Enter a message: \")\n",
    "    response = generate_response(user_input)\n",
    "    print(\"ChatBot: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For more information visit: https://medium.com/@fatih969692/building-a-simple-chatbot-using-deep-learning-2030cf58a231"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
